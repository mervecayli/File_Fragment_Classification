{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y2ZhgdAghP4L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from openpyxl import Workbook\n",
        "from openpyxl import load_workbook\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from optparse import OptionParser\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import bz2\n",
        "import random\n",
        "import math\n",
        "import rarfile\n",
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oMJIVb0NhVSu"
      },
      "outputs": [],
      "source": [
        "wb = load_workbook('20ktrain.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WmTwl4mahXis"
      },
      "outputs": [],
      "source": [
        "ws = wb.active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JzbvKXluhZLC"
      },
      "outputs": [],
      "source": [
        "review=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GQQnXWkahash"
      },
      "outputs": [],
      "source": [
        "# We are throwing our file paths into the review\n",
        "for satir in range(1,13001):\n",
        "    dosyaYolu= str(ws.cell(satir,1).value)\n",
        "    #print(dosyaYolu)\n",
        "    review.append(dosyaYolu)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0TDR3I2BhfeU"
      },
      "outputs": [],
      "source": [
        "entUni = []\n",
        "entBig=[]\n",
        "compr = []\n",
        "hw=[]\n",
        "cn=[]\n",
        "mbv=[]\n",
        "kg=[]\n",
        "ent=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bSJ1My1yhqIR"
      },
      "outputs": [],
      "source": [
        "def saveData(file_name,review,entUni,entBig,compr,hw,cn,mbv,kg,ent):\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    i=1\n",
        "    for count in range(0,13000):\n",
        "        ws['A'+str(i)] = review[count]\n",
        "        ws['B'+str(i)] = entUni[count]\n",
        "        ws['C'+str(i)] = entBig[count]\n",
        "        ws['D'+str(i)] = compr[count]\n",
        "        ws['E'+str(i)] = hw[count]\n",
        "        ws['F'+str(i)] = cn[count]\n",
        "        ws['G'+str(i)] = mbv[count]\n",
        "        ws['H'+str(i)] = kg[count]\n",
        "        ws['I'+str(i)] = ent[count]\n",
        "        i += 1\n",
        "    wb.save((file_name+\".xlsx\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unigram_entropy(review):\n",
        "    for i in review:\n",
        " # Create a dictionary to store the counts of each character in the string\n",
        "      counts = defaultdict(int)\n",
        "      for byte in i:       \n",
        "          counts[byte] += 1\n",
        "    # Create a list of the frequencies of each character, using the counts in the dictionary\n",
        "      unigram_frequencies = [counts[chr(byte)] for byte in range(255)]\n",
        "    \n",
        "      entropy = 0.0\n",
        "      for i in range(len(unigram_frequencies)):\n",
        "          if unigram_frequencies[i] > 0.0:\n",
        "              entropy += unigram_frequencies[i] * math.log10(unigram_frequencies[i])\n",
        "      entropy = -entropy\n",
        "    \n",
        "      entUni.append(entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy_and_bigram_counts(review):\n",
        "    for i in review:\n",
        "    # Create a dictionary to store the counts of each character in the string\n",
        "      counts = defaultdict(int)\n",
        "    # Iterate through each bigram in the string\n",
        "      for j in range(len(i)-1):\n",
        "          counts[i[j]+i[j+1]] += 1\n",
        "        \n",
        "      bigram_frequencies = [counts[chr(b1)+chr(b2)] for b1 in range(255) for b2 in range(255)]\n",
        "    \n",
        "      entropy = 0.0\n",
        "    \n",
        "      for i in range(len(bigram_frequencies)):\n",
        "          if bigram_frequencies[i] > 0.0:\n",
        "              entropy += bigram_frequencies[i] * math.log10(bigram_frequencies[i])\n",
        "      entropy = -entropy\n",
        "    \n",
        "    \n",
        "      entBig.append(entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compressed_length(review):\n",
        "    for i in review:\n",
        "      # Open the file and read in the data\n",
        "      with open(i, 'rb') as f:\n",
        "          data = f.read()\n",
        "          \n",
        "   # Calculate the ratio of the compressed data length to the original data length\n",
        "      cmp =  float( len(bz2.compress(data)) ) / float(len(data))\n",
        "     \n",
        "      compr.append(cmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vTdF8S8Xhs5S"
      },
      "outputs": [],
      "source": [
        "def hamming_weight(review):\n",
        "      \n",
        "    for i in review:\n",
        "        with open(i, 'rb') as file:\n",
        "            \n",
        "            file_data = file.read()\n",
        "\n",
        "        hamming_weight = 0.0\n",
        "        for i in range(len(file_data)):\n",
        "            current_byte = file_data[i]\n",
        "        # For each byte, add the number of bits that are set to 1\n",
        "            while current_byte != 0:\n",
        "                hamming_weight += float(current_byte & 1)\n",
        "                current_byte = current_byte >> 1\n",
        "        hamming_weight /= float(8 * len(file_data))\n",
        "\n",
        "        hw.append(hamming_weight)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eAH7ZnXNn47s"
      },
      "outputs": [],
      "source": [
        "def contiguity(review):\n",
        "  for i in review:\n",
        "    total_diff = 0\n",
        "    total = 0\n",
        "    for j in range(len(i)-1):\n",
        "      # Add the difference between the ASCII values of the current and next characters to total_diff\n",
        "        total_diff += abs(ord(i[j]) - ord(i[j+1]))\n",
        "        total += 1\n",
        "    k=total_diff/(total+0.0)\n",
        "    cn.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Lux9F3_AoCns"
      },
      "outputs": [],
      "source": [
        "def mean_byte_value(review):\n",
        "  for i in review:\n",
        "  # Calculate the sum of the ASCII values of each character in the word\n",
        "    k=sum([ord(char) for char in i])\n",
        "    mbv.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lqv4At50o7Cj"
      },
      "outputs": [],
      "source": [
        "def kolmogorov_complexity(review):\n",
        "  for i in review:\n",
        "\n",
        "    with open(i, \"rb\") as f:\n",
        "      content = f.read()\n",
        "    # Calculate the complexity of the file's content using the formula: len(content) * log(len(set(content)))\n",
        "      complexity = len(content) * log(len(set(content)))\n",
        "    kg.append(complexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy(review):\n",
        "  for i in review:\n",
        "\n",
        "    with open(i, 'rb') as f:\n",
        "      data = f.read()\n",
        "\n",
        "# Count the occurrences of each data point\n",
        "    data_count = {}\n",
        "    for ch in data:\n",
        "      if ch not in data_count:\n",
        "        data_count[ch] = 1\n",
        "      else:\n",
        "        data_count[ch] += 1\n",
        "\n",
        "# Calculate the frequency of each data point\n",
        "    data_prob = {}\n",
        "    for key, value in data_count.items():\n",
        "      data_prob[key] = value / len(data)\n",
        "\n",
        "# Calculate the entropy of the file\n",
        "    entropy = 0\n",
        "    for key, value in data_prob.items():\n",
        "      entropy += -value * math.log2(value)\n",
        "\n",
        "\n",
        "    ent.append(entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "unigram_entropy(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "entropy_and_bigram_counts(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressed_length(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "hamming_weight(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "contiguity(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_byte_value(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "kolmogorov_complexity(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "entropy(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "saveData(\"13ktrain\",review,entUni,entBig,compr,hw,cn,mbv,kg,ent)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Mlbase",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c70345758611b28261ace86a56dc1530a406f4da83e80d17d59b11ce218e72b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
